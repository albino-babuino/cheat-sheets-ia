{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scikit-learn - Preprocesamiento de Datos\n",
        "\n",
        "Este notebook cubre las técnicas de preprocesamiento de datos en scikit-learn: escalado, normalización, codificación de variables categóricas y manejo de valores faltantes.\n",
        "\n",
        "## Importar Módulos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Escalado de Datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Datos de ejemplo\n",
        "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "\n",
        "# StandardScaler: estandarización (media=0, desv. estándar=1)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(\"StandardScaler:\")\n",
        "print(f\"Media: {X_scaled.mean(axis=0)}\")\n",
        "print(f\"Desv. estándar: {X_scaled.std(axis=0)}\")\n",
        "\n",
        "# MinMaxScaler: normalización a rango [0, 1]\n",
        "minmax = MinMaxScaler()\n",
        "X_minmax = minmax.fit_transform(X)\n",
        "print(\"\\nMinMaxScaler (rango [0,1]):\")\n",
        "print(X_minmax)\n",
        "\n",
        "# RobustScaler: usa mediana y rango intercuartílico (robusto a outliers)\n",
        "robust = RobustScaler()\n",
        "X_robust = robust.fit_transform(X)\n",
        "print(\"\\nRobustScaler:\")\n",
        "print(X_robust)\n",
        "\n",
        "# Normalizer: normaliza cada muestra (fila) a norma unitaria\n",
        "normalizer = Normalizer()\n",
        "X_norm = normalizer.fit_transform(X)\n",
        "print(\"\\nNormalizer (norma unitaria por fila):\")\n",
        "print(X_norm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Codificación de Variables Categóricas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LabelEncoder: codifica etiquetas a números (0, 1, 2, ...)\n",
        "labels = ['rojo', 'verde', 'azul', 'rojo', 'verde']\n",
        "le = LabelEncoder()\n",
        "labels_encoded = le.fit_transform(labels)\n",
        "print(\"LabelEncoder:\")\n",
        "print(f\"Original: {labels}\")\n",
        "print(f\"Codificado: {labels_encoded}\")\n",
        "print(f\"Clases: {le.classes_}\")\n",
        "\n",
        "# OneHotEncoder: codificación one-hot (binaria)\n",
        "categories = [['A', 'B', 'C'], ['X', 'Y']]\n",
        "ohe = OneHotEncoder()\n",
        "data_cat = [['A', 'X'], ['B', 'Y'], ['C', 'X']]\n",
        "data_encoded = ohe.fit_transform(data_cat).toarray()\n",
        "print(\"\\nOneHotEncoder:\")\n",
        "print(data_encoded)\n",
        "print(f\"Nombres de características: {ohe.get_feature_names_out()}\")\n",
        "\n",
        "# OrdinalEncoder: codificación ordinal (similar a LabelEncoder pero para múltiples columnas)\n",
        "oe = OrdinalEncoder()\n",
        "data_ordinal = oe.fit_transform(data_cat)\n",
        "print(\"\\nOrdinalEncoder:\")\n",
        "print(data_ordinal)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Manejo de Valores Faltantes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Datos con valores faltantes\n",
        "X_missing = np.array([[1, 2, np.nan], [4, np.nan, 6], [7, 8, 9]])\n",
        "\n",
        "# SimpleImputer: imputación de valores faltantes\n",
        "# Estrategias: 'mean', 'median', 'most_frequent', 'constant'\n",
        "imputer_mean = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer_mean.fit_transform(X_missing)\n",
        "print(\"SimpleImputer (media):\")\n",
        "print(X_imputed)\n",
        "\n",
        "imputer_median = SimpleImputer(strategy='median')\n",
        "X_imputed_median = imputer_median.fit_transform(X_missing)\n",
        "print(\"\\nSimpleImputer (mediana):\")\n",
        "print(X_imputed_median)\n",
        "\n",
        "imputer_constant = SimpleImputer(strategy='constant', fill_value=0)\n",
        "X_imputed_const = imputer_constant.fit_transform(X_missing)\n",
        "print(\"\\nSimpleImputer (constante=0):\")\n",
        "print(X_imputed_const)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformaciones Polinómicas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PolynomialFeatures: genera características polinómicas\n",
        "X = np.array([[2, 3], [4, 5]])\n",
        "\n",
        "# Grado 2\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly.fit_transform(X)\n",
        "print(\"PolynomialFeatures (grado 2):\")\n",
        "print(f\"Original shape: {X.shape}\")\n",
        "print(f\"Polinómico shape: {X_poly.shape}\")\n",
        "print(X_poly)\n",
        "print(f\"Nombres: {poly.get_feature_names_out()}\")\n",
        "\n",
        "# Con sesgo (bias)\n",
        "poly_bias = PolynomialFeatures(degree=2, include_bias=True)\n",
        "X_poly_bias = poly_bias.fit_transform(X)\n",
        "print(\"\\nCon bias (columna de unos):\")\n",
        "print(X_poly_bias)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline de Preprocesamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Crear un pipeline de preprocesamiento\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('poly', PolynomialFeatures(degree=2, include_bias=False))\n",
        "])\n",
        "\n",
        "# Aplicar pipeline\n",
        "X_example = np.array([[1, 2], [3, np.nan], [5, 6]])\n",
        "X_processed = pipeline.fit_transform(X_example)\n",
        "print(\"Pipeline de preprocesamiento:\")\n",
        "print(f\"Shape original: {X_example.shape}\")\n",
        "print(f\"Shape procesado: {X_processed.shape}\")\n",
        "print(X_processed)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
