{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Redes Neuronales Recurrentes (RNN) y LSTM\n",
        "\n",
        "Este notebook cubre las Redes Neuronales Recurrentes (RNN) y Long Short-Term Memory (LSTM), fundamentales para procesamiento de secuencias y series temporales.\n",
        "\n",
        "## Introducción\n",
        "\n",
        "Las RNN están diseñadas para trabajar con datos secuenciales donde el orden importa. Mantienen un estado oculto que captura información de pasos anteriores.\n",
        "\n",
        "### Conceptos Clave\n",
        "\n",
        "- **RNN**: Red que procesa secuencias manteniendo memoria de estados anteriores\n",
        "- **LSTM**: Variante de RNN que resuelve el problema del gradiente desaparecido\n",
        "- **GRU**: Gated Recurrent Unit, versión simplificada de LSTM\n",
        "- **Backpropagation Through Time (BPTT)**: Algoritmo de entrenamiento para RNN\n",
        "- **Vanishing Gradient**: Problema que limita el aprendizaje de dependencias a largo plazo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importar Librerías\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Intentar importar TensorFlow/Keras\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers, models\n",
        "    TENSORFLOW_AVAILABLE = True\n",
        "    print(f\"TensorFlow versión: {tf.__version__}\")\n",
        "except ImportError:\n",
        "    TENSORFLOW_AVAILABLE = False\n",
        "    print(\"TensorFlow no está instalado. Instala con: pip install tensorflow\")\n",
        "\n",
        "np.random.seed(42)\n",
        "if TENSORFLOW_AVAILABLE:\n",
        "    tf.random.set_seed(42)\n",
        "    \n",
        "# Configuración de visualización\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "except:\n",
        "    plt.style.use('seaborn')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. RNN Básica desde Cero\n",
        "\n",
        "Implementemos una RNN simple para entender cómo funciona el procesamiento de secuencias.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleRNN:\n",
        "    \"\"\"RNN básica para procesamiento de secuencias\"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        # Pesos\n",
        "        self.W_xh = np.random.randn(input_size, hidden_size) * 0.1\n",
        "        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.1\n",
        "        self.W_hy = np.random.randn(hidden_size, output_size) * 0.1\n",
        "        \n",
        "        # Biases\n",
        "        self.b_h = np.zeros((1, hidden_size))\n",
        "        self.b_y = np.zeros((1, output_size))\n",
        "    \n",
        "    def tanh(self, x):\n",
        "        \"\"\"Función de activación tanh\"\"\"\n",
        "        return np.tanh(np.clip(x, -250, 250))\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"Propagación hacia adelante\"\"\"\n",
        "        # X shape: (seq_length, batch_size, input_size)\n",
        "        seq_length, batch_size, _ = X.shape\n",
        "        \n",
        "        self.hidden_states = [np.zeros((batch_size, self.hidden_size))]\n",
        "        self.outputs = []\n",
        "        \n",
        "        for t in range(seq_length):\n",
        "            # Calcular estado oculto\n",
        "            h_t = self.tanh(\n",
        "                np.dot(X[t], self.W_xh) + \n",
        "                np.dot(self.hidden_states[-1], self.W_hh) + \n",
        "                self.b_h\n",
        "            )\n",
        "            self.hidden_states.append(h_t)\n",
        "            \n",
        "            # Calcular salida\n",
        "            y_t = np.dot(h_t, self.W_hy) + self.b_y\n",
        "            self.outputs.append(y_t)\n",
        "        \n",
        "        return np.array(self.outputs)\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Realizar predicción\"\"\"\n",
        "        outputs = self.forward(X)\n",
        "        return outputs[-1]  # Retornar última salida\n",
        "\n",
        "# Ejemplo: Predicción de secuencia simple\n",
        "# Entrenar a predecir el siguiente número en una secuencia\n",
        "seq_length = 5\n",
        "batch_size = 1\n",
        "input_size = 1\n",
        "hidden_size = 10\n",
        "output_size = 1\n",
        "\n",
        "rnn = SimpleRNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Secuencia de ejemplo: [1, 2, 3, 4, 5] -> predecir 6\n",
        "X = np.array([[[1]], [[2]], [[3]], [[4]], [[5]]])  # (seq_length, batch, input)\n",
        "y_target = np.array([[6]])\n",
        "\n",
        "# Forward pass\n",
        "outputs = rnn.forward(X)\n",
        "print(f\"Secuencia de entrada: {X.flatten()}\")\n",
        "print(f\"Salida predicha: {outputs[-1][0][0]:.4f}\")\n",
        "print(f\"Valor objetivo: {y_target[0][0]}\")\n",
        "\n",
        "# Visualización de estados ocultos\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
        "hidden_array = np.array([h[0] for h in rnn.hidden_states[1:]])\n",
        "ax.plot(hidden_array)\n",
        "ax.set_xlabel('Paso de Tiempo')\n",
        "ax.set_ylabel('Valor del Estado Oculto')\n",
        "ax.set_title('Evolución de los Estados Ocultos de la RNN')\n",
        "ax.legend([f'Neurona {i+1}' for i in range(hidden_size)])\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. LSTM con TensorFlow/Keras\n",
        "\n",
        "Las LSTM resuelven el problema del gradiente desaparecido usando puertas (gates) que controlan el flujo de información.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TENSORFLOW_AVAILABLE:\n",
        "    # Generar datos de serie temporal sintética\n",
        "    time_steps = 100\n",
        "    t = np.linspace(0, 4*np.pi, time_steps)\n",
        "    data = np.sin(t) + 0.1 * np.random.randn(time_steps)\n",
        "    \n",
        "    # Preparar datos para predicción de secuencia\n",
        "    def create_sequences(data, seq_length):\n",
        "        X, y = [], []\n",
        "        for i in range(len(data) - seq_length):\n",
        "            X.append(data[i:i+seq_length])\n",
        "            y.append(data[i+seq_length])\n",
        "        return np.array(X), np.array(y)\n",
        "    \n",
        "    seq_length = 10\n",
        "    X, y = create_sequences(data, seq_length)\n",
        "    \n",
        "    # Normalizar\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X.T).T\n",
        "    y_scaled = scaler.transform(y.reshape(-1, 1)).flatten()\n",
        "    \n",
        "    # Reshape para LSTM: (samples, time_steps, features)\n",
        "    X_scaled = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1)\n",
        "    \n",
        "    # Dividir en train/test\n",
        "    split = int(0.8 * len(X_scaled))\n",
        "    X_train, X_test = X_scaled[:split], X_scaled[split:]\n",
        "    y_train, y_test = y_scaled[:split], y_scaled[split:]\n",
        "    \n",
        "    # Construir modelo LSTM\n",
        "    model = models.Sequential([\n",
        "        layers.LSTM(50, activation='relu', input_shape=(seq_length, 1), return_sequences=True),\n",
        "        layers.LSTM(50, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    \n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    \n",
        "    print(\"Arquitectura del modelo LSTM:\")\n",
        "    model.summary()\n",
        "    \n",
        "    # Entrenar\n",
        "    print(\"\\nEntrenando modelo LSTM...\")\n",
        "    history = model.fit(X_train, y_train, \n",
        "                       epochs=20, \n",
        "                       batch_size=16,\n",
        "                       validation_split=0.2,\n",
        "                       verbose=1)\n",
        "    \n",
        "    # Predicciones\n",
        "    train_pred = model.predict(X_train, verbose=0)\n",
        "    test_pred = model.predict(X_test, verbose=0)\n",
        "    \n",
        "    # Desnormalizar\n",
        "    train_pred = scaler.inverse_transform(train_pred)\n",
        "    test_pred = scaler.inverse_transform(test_pred)\n",
        "    y_train_orig = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
        "    y_test_orig = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "    \n",
        "    # Visualización\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
        "    \n",
        "    # Datos originales y predicciones\n",
        "    train_indices = range(seq_length, seq_length + len(train_pred))\n",
        "    test_indices = range(seq_length + len(train_pred), seq_length + len(train_pred) + len(test_pred))\n",
        "    \n",
        "    axes[0].plot(data, label='Datos Originales', alpha=0.7)\n",
        "    axes[0].plot(train_indices, train_pred, label='Predicciones (Train)', linestyle='--')\n",
        "    axes[0].plot(test_indices, test_pred, label='Predicciones (Test)', linestyle='--', color='red')\n",
        "    axes[0].axvline(split + seq_length, color='green', linestyle=':', label='División Train/Test')\n",
        "    axes[0].set_xlabel('Tiempo')\n",
        "    axes[0].set_ylabel('Valor')\n",
        "    axes[0].set_title('Predicción de Serie Temporal con LSTM')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Curva de aprendizaje\n",
        "    axes[1].plot(history.history['loss'], label='Pérdida (Train)')\n",
        "    axes[1].plot(history.history['val_loss'], label='Pérdida (Validación)')\n",
        "    axes[1].set_xlabel('Época')\n",
        "    axes[1].set_ylabel('Pérdida (MSE)')\n",
        "    axes[1].set_title('Curva de Aprendizaje')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Calcular métricas\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "    \n",
        "    train_mse = mean_squared_error(y_train_orig, train_pred)\n",
        "    test_mse = mean_squared_error(y_test_orig, test_pred)\n",
        "    train_mae = mean_absolute_error(y_train_orig, train_pred)\n",
        "    test_mae = mean_absolute_error(y_test_orig, test_pred)\n",
        "    \n",
        "    print(f\"\\nMétricas:\")\n",
        "    print(f\"Train MSE: {train_mse:.4f}, MAE: {train_mae:.4f}\")\n",
        "    print(f\"Test MSE: {test_mse:.4f}, MAE: {test_mae:.4f}\")\n",
        "    \n",
        "else:\n",
        "    print(\"TensorFlow no está disponible. Instala con: pip install tensorflow\")\n",
        "    print(\"\\nEjemplo de arquitectura LSTM típica:\")\n",
        "    print(\"\"\"\n",
        "    Input (seq_length, features)\n",
        "        ↓\n",
        "    LSTM (50 unidades, return_sequences=True)\n",
        "        ↓\n",
        "    LSTM (50 unidades)\n",
        "        ↓\n",
        "    Dense (1)\n",
        "        ↓\n",
        "    Output (predicción)\n",
        "    \"\"\")\n",
        "    print(\"\\nComponentes de una celda LSTM:\")\n",
        "    print(\"- Forget Gate: Decide qué información olvidar\")\n",
        "    print(\"- Input Gate: Decide qué nueva información almacenar\")\n",
        "    print(\"- Cell State: Transporta información a través de la secuencia\")\n",
        "    print(\"- Output Gate: Decide qué parte del cell state usar como salida\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TENSORFLOW_AVAILABLE:\n",
        "    # Crear modelos comparativos\n",
        "    models_dict = {\n",
        "        'SimpleRNN': models.Sequential([\n",
        "            layers.SimpleRNN(32, input_shape=(seq_length, 1)),\n",
        "            layers.Dense(1)\n",
        "        ]),\n",
        "        'LSTM': models.Sequential([\n",
        "            layers.LSTM(32, input_shape=(seq_length, 1)),\n",
        "            layers.Dense(1)\n",
        "        ]),\n",
        "        'GRU': models.Sequential([\n",
        "            layers.GRU(32, input_shape=(seq_length, 1)),\n",
        "            layers.Dense(1)\n",
        "        ])\n",
        "    }\n",
        "    \n",
        "    # Compilar y entrenar cada modelo\n",
        "    histories = {}\n",
        "    for name, model in models_dict.items():\n",
        "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "        print(f\"\\nEntrenando {name}...\")\n",
        "        history = model.fit(X_train, y_train, \n",
        "                           epochs=15, \n",
        "                           batch_size=16,\n",
        "                           validation_split=0.2,\n",
        "                           verbose=0)\n",
        "        histories[name] = history\n",
        "    \n",
        "    # Visualización comparativa\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Comparar pérdidas\n",
        "    for name, history in histories.items():\n",
        "        axes[0].plot(history.history['val_loss'], label=f'{name} (Val)')\n",
        "    axes[0].set_xlabel('Época')\n",
        "    axes[0].set_ylabel('Pérdida (MSE)')\n",
        "    axes[0].set_title('Comparación de Pérdidas (Validación)')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Comparar MAE\n",
        "    for name, history in histories.items():\n",
        "        axes[1].plot(history.history['val_mae'], label=f'{name} (Val)')\n",
        "    axes[1].set_xlabel('Época')\n",
        "    axes[1].set_ylabel('MAE')\n",
        "    axes[1].set_title('Comparación de MAE (Validación)')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Comparar número de parámetros\n",
        "    print(\"\\nComparación de parámetros:\")\n",
        "    for name, model in models_dict.items():\n",
        "        total_params = model.count_params()\n",
        "        print(f\"{name}: {total_params:,} parámetros\")\n",
        "    \n",
        "else:\n",
        "    print(\"TensorFlow no está disponible.\")\n",
        "    print(\"\\nComparación de arquitecturas:\")\n",
        "    print(\"\"\"\n",
        "    SimpleRNN:\n",
        "    - Más simple y rápida\n",
        "    - Problema de gradiente desaparecido\n",
        "    - Buena para secuencias cortas\n",
        "    \n",
        "    LSTM:\n",
        "    - Resuelve el problema del gradiente desaparecido\n",
        "    - 3 puertas (forget, input, output)\n",
        "    - Mejor para dependencias a largo plazo\n",
        "    - Más parámetros que RNN\n",
        "    \n",
        "    GRU:\n",
        "    - Versión simplificada de LSTM\n",
        "    - 2 puertas (reset, update)\n",
        "    - Menos parámetros que LSTM\n",
        "    - Rendimiento similar a LSTM en muchos casos\n",
        "    \"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Resumen\n",
        "\n",
        "### Diferencias Clave\n",
        "\n",
        "**RNN Simple:**\n",
        "- Mantiene un estado oculto simple\n",
        "- Problema del gradiente desaparecido\n",
        "- Eficiente pero limitada\n",
        "\n",
        "**LSTM:**\n",
        "- Cell state y hidden state separados\n",
        "- 3 puertas para controlar flujo de información\n",
        "- Excelente para dependencias a largo plazo\n",
        "\n",
        "**GRU:**\n",
        "- Combinación de cell state y hidden state\n",
        "- 2 puertas (más simple que LSTM)\n",
        "- Buen equilibrio entre complejidad y rendimiento\n",
        "\n",
        "### Aplicaciones\n",
        "\n",
        "- **Procesamiento de lenguaje natural**: Traducción, generación de texto\n",
        "- **Series temporales**: Predicción de valores futuros\n",
        "- **Reconocimiento de voz**: Transcripción de audio\n",
        "- **Análisis de sentimientos**: Clasificación de texto\n",
        "- **Modelado de secuencias**: Música, video\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
