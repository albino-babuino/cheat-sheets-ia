{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Naive Bayes\n",
        "\n",
        "Este notebook explica el algoritmo Naive Bayes, un clasificador probabilístico basado en el teorema de Bayes.\n",
        "\n",
        "## ¿Qué es Naive Bayes?\n",
        "\n",
        "Naive Bayes es un algoritmo de clasificación basado en el teorema de Bayes con el supuesto de \"naive\" (ingenuo) de independencia entre las características. A pesar de esta simplificación, funciona sorprendentemente bien en muchos problemas del mundo real.\n",
        "\n",
        "## Conceptos Fundamentales\n",
        "\n",
        "- **Teorema de Bayes**: P(A|B) = P(B|A) × P(A) / P(B)\n",
        "- **Supuesto de independencia**: Las características son independientes entre sí\n",
        "- **Probabilidad a priori**: P(clase) - probabilidad de cada clase\n",
        "- **Probabilidad condicional**: P(característica|clase) - probabilidad de una característica dado una clase\n",
        "- **Probabilidad a posteriori**: P(clase|características) - probabilidad de una clase dado las características\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Teorema de Bayes\n",
        "\n",
        "El teorema de Bayes nos permite calcular la probabilidad de una clase dado un conjunto de características:\n",
        "\n",
        "$$P(C|X) = \\frac{P(X|C) \\times P(C)}{P(X)}$$\n",
        "\n",
        "Donde:\n",
        "- $P(C|X)$: Probabilidad a posteriori (clase dado características)\n",
        "- $P(X|C)$: Probabilidad condicional (características dado clase)\n",
        "- $P(C)$: Probabilidad a priori (clase)\n",
        "- $P(X)$: Probabilidad de las características (normalización)\n",
        "\n",
        "## Supuesto Naive (Ingenuo)\n",
        "\n",
        "El supuesto de independencia permite simplificar el cálculo:\n",
        "\n",
        "$$P(X|C) = P(x_1|C) \\times P(x_2|C) \\times ... \\times P(x_n|C)$$\n",
        "\n",
        "Esto significa que asumimos que las características son independientes entre sí, lo cual rara vez es cierto en la práctica, pero el algoritmo funciona bien de todos modos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NaiveBayesClassifier:\n",
        "    \"\"\"Implementación básica de Naive Bayes para clasificación\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.classes = None\n",
        "        self.class_priors = {}\n",
        "        self.class_likelihoods = {}\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Entrena el clasificador Naive Bayes\"\"\"\n",
        "        self.classes = np.unique(y)\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Calcular probabilidades a priori\n",
        "        for c in self.classes:\n",
        "            self.class_priors[c] = np.sum(y == c) / n_samples\n",
        "        \n",
        "        # Calcular probabilidades condicionales (likelihoods)\n",
        "        for c in self.classes:\n",
        "            X_c = X[y == c]\n",
        "            self.class_likelihoods[c] = {}\n",
        "            \n",
        "            for feature_idx in range(n_features):\n",
        "                feature_values = X_c[:, feature_idx]\n",
        "                # Asumir distribución normal (Gaussian)\n",
        "                mean = np.mean(feature_values)\n",
        "                std = np.std(feature_values)\n",
        "                self.class_likelihoods[c][feature_idx] = {'mean': mean, 'std': std}\n",
        "    \n",
        "    def gaussian_pdf(self, x, mean, std):\n",
        "        \"\"\"Función de densidad de probabilidad gaussiana\"\"\"\n",
        "        if std == 0:\n",
        "            return 1.0 if x == mean else 0.0\n",
        "        return (1.0 / (np.sqrt(2 * np.pi) * std)) * np.exp(-0.5 * ((x - mean) / std) ** 2)\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Predice las clases para los puntos en X\"\"\"\n",
        "        predictions = []\n",
        "        \n",
        "        for x in X:\n",
        "            posteriors = {}\n",
        "            \n",
        "            for c in self.classes:\n",
        "                # Iniciar con la probabilidad a priori\n",
        "                posterior = np.log(self.class_priors[c])\n",
        "                \n",
        "                # Multiplicar por las probabilidades condicionales (usando log para evitar underflow)\n",
        "                for feature_idx in range(len(x)):\n",
        "                    mean = self.class_likelihoods[c][feature_idx]['mean']\n",
        "                    std = self.class_likelihoods[c][feature_idx]['std']\n",
        "                    likelihood = self.gaussian_pdf(x[feature_idx], mean, std)\n",
        "                    # Usar log para evitar problemas numéricos\n",
        "                    if likelihood > 0:\n",
        "                        posterior += np.log(likelihood)\n",
        "                \n",
        "                posteriors[c] = posterior\n",
        "            \n",
        "            # Clase con mayor probabilidad a posteriori\n",
        "            predicted_class = max(posteriors, key=posteriors.get)\n",
        "            predictions.append(predicted_class)\n",
        "        \n",
        "        return np.array(predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tipos de Naive Bayes\n",
        "\n",
        "Existen diferentes variantes según el tipo de datos:\n",
        "\n",
        "1. **Gaussian Naive Bayes**: Para características continuas (distribución normal)\n",
        "2. **Multinomial Naive Bayes**: Para conteos/discretos (texto, clasificación de documentos)\n",
        "3. **Bernoulli Naive Bayes**: Para características binarias (presencia/ausencia)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar dataset de iris\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Dividir en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Usar nuestra implementación\n",
        "nb_custom = NaiveBayesClassifier()\n",
        "nb_custom.fit(X_train, y_train)\n",
        "y_pred_custom = nb_custom.predict(X_test)\n",
        "\n",
        "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
        "print(f\"Precisión con implementación propia: {accuracy_custom:.4f}\")\n",
        "\n",
        "# Usar scikit-learn (Gaussian Naive Bayes)\n",
        "nb_sklearn = GaussianNB()\n",
        "nb_sklearn.fit(X_train, y_train)\n",
        "y_pred_sklearn = nb_sklearn.predict(X_test)\n",
        "\n",
        "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
        "print(f\"Precisión con scikit-learn (GaussianNB): {accuracy_sklearn:.4f}\")\n",
        "\n",
        "print(\"\\nReporte de clasificación:\")\n",
        "print(classification_report(y_test, y_pred_sklearn, target_names=iris.target_names))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Matriz de confusión\n",
        "cm = confusion_matrix(y_test, y_pred_sklearn)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=iris.target_names,\n",
        "            yticklabels=iris.target_names)\n",
        "plt.title('Matriz de Confusión - Naive Bayes')\n",
        "plt.ylabel('Clase Real')\n",
        "plt.xlabel('Clase Predicha')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparación de Variantes de Naive Bayes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparar diferentes variantes\n",
        "classifiers = {\n",
        "    'GaussianNB': GaussianNB(),\n",
        "    'MultinomialNB': MultinomialNB(),\n",
        "    'BernoulliNB': BernoulliNB()\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, clf in classifiers.items():\n",
        "    clf.fit(X_train, y_train)\n",
        "    score = clf.score(X_test, y_test)\n",
        "    results[name] = score\n",
        "    print(f\"{name}: {score:.4f}\")\n",
        "\n",
        "# Visualizar\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(results.keys(), results.values(), color=['skyblue', 'lightgreen', 'salmon'])\n",
        "plt.ylabel('Precisión')\n",
        "plt.title('Comparación de Variantes de Naive Bayes')\n",
        "plt.ylim([min(results.values()) - 0.05, max(results.values()) + 0.05])\n",
        "for name, score in results.items():\n",
        "    plt.text(name, score, f'{score:.4f}', ha='center', va='bottom')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejemplo: Clasificación de Texto\n",
        "\n",
        "Naive Bayes es especialmente útil para clasificación de texto:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Ejemplo simple de clasificación de texto\n",
        "texts = [\n",
        "    \"me encanta este producto\",\n",
        "    \"odio este servicio\",\n",
        "    \"excelente calidad\",\n",
        "    \"muy malo\",\n",
        "    \"recomiendo totalmente\",\n",
        "    \"no lo compren\",\n",
        "    \"súper bueno\",\n",
        "    \"terrible experiencia\"\n",
        "]\n",
        "\n",
        "labels = [1, 0, 1, 0, 1, 0, 1, 0]  # 1 = positivo, 0 = negativo\n",
        "\n",
        "# Convertir texto a vectores de características\n",
        "vectorizer = CountVectorizer()\n",
        "X_text = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Entrenar Naive Bayes\n",
        "nb_text = MultinomialNB()\n",
        "nb_text.fit(X_text, labels)\n",
        "\n",
        "# Predecir nuevos textos\n",
        "new_texts = [\"me gusta mucho\", \"es horrible\"]\n",
        "X_new = vectorizer.transform(new_texts)\n",
        "predictions = nb_text.predict(X_new)\n",
        "\n",
        "for text, pred in zip(new_texts, predictions):\n",
        "    sentiment = \"Positivo\" if pred == 1 else \"Negativo\"\n",
        "    print(f\"'{text}' -> {sentiment}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ventajas y Desventajas\n",
        "\n",
        "**Ventajas:**\n",
        "- Simple y rápido de entrenar\n",
        "- Funciona bien con pocos datos\n",
        "- Maneja múltiples clases naturalmente\n",
        "- No es sensible a características irrelevantes\n",
        "- Excelente para clasificación de texto\n",
        "\n",
        "**Desventajas:**\n",
        "- Supuesto de independencia rara vez se cumple en la práctica\n",
        "- Puede tener problemas con datos continuos si no se asume distribución correcta\n",
        "- Sensible a características correlacionadas\n",
        "\n",
        "## Aplicaciones\n",
        "\n",
        "- Clasificación de texto (spam, análisis de sentimientos)\n",
        "- Sistemas de recomendación\n",
        "- Diagnóstico médico\n",
        "- Filtrado de contenido\n",
        "- Clasificación de documentos\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
