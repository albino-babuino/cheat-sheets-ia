{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Regresión Lineal desde Cero\n",
        "\n",
        "Este notebook explica la implementación de Regresión Lineal desde cero, uno de los algoritmos más fundamentales en Machine Learning.\n",
        "\n",
        "## ¿Qué es Regresión Lineal?\n",
        "\n",
        "La Regresión Lineal es un algoritmo de aprendizaje supervisado que modela la relación entre una variable dependiente (y) y una o más variables independientes (X) usando una función lineal.\n",
        "\n",
        "## Conceptos Fundamentales\n",
        "\n",
        "- **Función lineal**: $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n$\n",
        "- **Coeficientes (β)**: Parámetros del modelo que se aprenden durante el entrenamiento\n",
        "- **Gradiente Descendente**: Algoritmo de optimización para encontrar los mejores coeficientes\n",
        "- **Error cuadrático medio (MSE)**: Función de costo que se minimiza\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ecuación de Regresión Lineal\n",
        "\n",
        "Para regresión lineal simple (una variable):\n",
        "\n",
        "$$y = \\beta_0 + \\beta_1 x$$\n",
        "\n",
        "Para regresión lineal múltiple (múltiples variables):\n",
        "\n",
        "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n$$\n",
        "\n",
        "En forma matricial:\n",
        "\n",
        "$$y = X\\beta$$\n",
        "\n",
        "Donde:\n",
        "- $X$ es la matriz de características (con columna de unos para el término de intercepto)\n",
        "- $\\beta$ es el vector de coeficientes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Método 1: Ecuación Normal (Solución Analítica)\n",
        "\n",
        "La solución analítica usando mínimos cuadrados:\n",
        "\n",
        "$$\\beta = (X^T X)^{-1} X^T y$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LinearRegressionNormal:\n",
        "    \"\"\"Regresión Lineal usando ecuación normal (solución analítica)\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.coefficients = None\n",
        "        self.intercept = None\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Entrena el modelo usando ecuación normal\"\"\"\n",
        "        # Añadir columna de unos para el término de intercepto\n",
        "        X_with_intercept = np.column_stack([np.ones(X.shape[0]), X])\n",
        "        \n",
        "        # Calcular coeficientes usando ecuación normal\n",
        "        # β = (X^T X)^(-1) X^T y\n",
        "        XTX = np.dot(X_with_intercept.T, X_with_intercept)\n",
        "        XTy = np.dot(X_with_intercept.T, y)\n",
        "        self.coefficients = np.linalg.solve(XTX, XTy)\n",
        "        \n",
        "        # Separar intercept y coeficientes\n",
        "        self.intercept = self.coefficients[0]\n",
        "        self.coefficients = self.coefficients[1:]\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Hace predicciones\"\"\"\n",
        "        return self.intercept + np.dot(X, self.coefficients)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Método 2: Gradiente Descendente\n",
        "\n",
        "Gradiente descendente es un algoritmo iterativo que minimiza la función de costo:\n",
        "\n",
        "$$\\text{Costo} = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\beta(x^{(i)}) - y^{(i)})^2$$\n",
        "\n",
        "Donde $h_\\beta(x) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n$ es la hipótesis.\n",
        "\n",
        "El gradiente se actualiza iterativamente:\n",
        "\n",
        "$$\\beta_j := \\beta_j - \\alpha \\frac{\\partial}{\\partial \\beta_j} \\text{Costo}(\\beta)$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LinearRegressionGradientDescent:\n",
        "    \"\"\"Regresión Lineal usando Gradiente Descendente\"\"\"\n",
        "    \n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
        "        \"\"\"\n",
        "        Parámetros:\n",
        "        - learning_rate: Tasa de aprendizaje (alpha)\n",
        "        - n_iterations: Número de iteraciones\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.coefficients = None\n",
        "        self.intercept = None\n",
        "        self.cost_history = []\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Entrena el modelo usando gradiente descendente\"\"\"\n",
        "        m, n = X.shape\n",
        "        \n",
        "        # Inicializar parámetros\n",
        "        self.coefficients = np.zeros(n)\n",
        "        self.intercept = 0\n",
        "        \n",
        "        # Normalizar características para mejor convergencia\n",
        "        X_normalized = (X - np.mean(X, axis=0)) / (np.std(X, axis=0) + 1e-8)\n",
        "        \n",
        "        # Gradiente descendente\n",
        "        for i in range(self.n_iterations):\n",
        "            # Predicciones\n",
        "            y_pred = self.intercept + np.dot(X_normalized, self.coefficients)\n",
        "            \n",
        "            # Calcular error\n",
        "            error = y_pred - y\n",
        "            \n",
        "            # Calcular costo (MSE)\n",
        "            cost = (1 / (2 * m)) * np.sum(error ** 2)\n",
        "            self.cost_history.append(cost)\n",
        "            \n",
        "            # Calcular gradientes\n",
        "            d_intercept = (1 / m) * np.sum(error)\n",
        "            d_coefficients = (1 / m) * np.dot(X_normalized.T, error)\n",
        "            \n",
        "            # Actualizar parámetros\n",
        "            self.intercept -= self.learning_rate * d_intercept\n",
        "            self.coefficients -= self.learning_rate * d_coefficients\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Hace predicciones\"\"\"\n",
        "        X_normalized = (X - np.mean(X, axis=0)) / (np.std(X, axis=0) + 1e-8)\n",
        "        return self.intercept + np.dot(X_normalized, self.coefficients)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejemplo: Regresión Lineal Simple\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generar datos sintéticos\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2.5 * X.flatten() + 1.5 + np.random.randn(100) * 2\n",
        "\n",
        "# Dividir en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Usar ecuación normal\n",
        "lr_normal = LinearRegressionNormal()\n",
        "lr_normal.fit(X_train, y_train)\n",
        "y_pred_normal = lr_normal.predict(X_test)\n",
        "\n",
        "# Usar gradiente descendente\n",
        "lr_gd = LinearRegressionGradientDescent(learning_rate=0.01, n_iterations=1000)\n",
        "lr_gd.fit(X_train, y_train)\n",
        "y_pred_gd = lr_gd.predict(X_test)\n",
        "\n",
        "# Usar scikit-learn\n",
        "lr_sklearn = LinearRegression()\n",
        "lr_sklearn.fit(X_train, y_train)\n",
        "y_pred_sklearn = lr_sklearn.predict(X_test)\n",
        "\n",
        "# Comparar resultados\n",
        "print(\"Ecuación Normal:\")\n",
        "print(f\"  Intercept: {lr_normal.intercept:.4f}, Coeficiente: {lr_normal.coefficients[0]:.4f}\")\n",
        "print(f\"  MSE: {mean_squared_error(y_test, y_pred_normal):.4f}\")\n",
        "print(f\"  R²: {r2_score(y_test, y_pred_normal):.4f}\")\n",
        "\n",
        "print(\"\\nGradiente Descendente:\")\n",
        "print(f\"  Intercept: {lr_gd.intercept:.4f}, Coeficiente: {lr_gd.coefficients[0]:.4f}\")\n",
        "print(f\"  MSE: {mean_squared_error(y_test, y_pred_gd):.4f}\")\n",
        "print(f\"  R²: {r2_score(y_test, y_pred_gd):.4f}\")\n",
        "\n",
        "print(\"\\nScikit-learn:\")\n",
        "print(f\"  Intercept: {lr_sklearn.intercept_:.4f}, Coeficiente: {lr_sklearn.coef_[0]:.4f}\")\n",
        "print(f\"  MSE: {mean_squared_error(y_test, y_pred_sklearn):.4f}\")\n",
        "print(f\"  R²: {r2_score(y_test, y_pred_sklearn):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizar regresión\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# Gráfico 1: Datos y línea de regresión\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_test, y_test, alpha=0.5, label='Datos reales')\n",
        "plt.plot(X_test, y_pred_normal, 'r-', linewidth=2, label='Ecuación Normal')\n",
        "plt.plot(X_test, y_pred_gd, 'g--', linewidth=2, label='Gradiente Descendente')\n",
        "plt.plot(X_test, y_pred_sklearn, 'b:', linewidth=2, label='Scikit-learn')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Regresión Lineal Simple')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Gráfico 2: Historial de costo (Gradiente Descendente)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(lr_gd.cost_history)\n",
        "plt.xlabel('Iteración')\n",
        "plt.ylabel('Costo (MSE)')\n",
        "plt.title('Convergencia del Gradiente Descendente')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regresión Lineal Múltiple\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejemplo con múltiples características\n",
        "X_multi, y_multi = datasets.make_regression(\n",
        "    n_samples=100, n_features=3, noise=10, random_state=42\n",
        ")\n",
        "\n",
        "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n",
        "    X_multi, y_multi, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Entrenar modelos\n",
        "lr_normal_multi = LinearRegressionNormal()\n",
        "lr_normal_multi.fit(X_train_multi, y_train_multi)\n",
        "y_pred_normal_multi = lr_normal_multi.predict(X_test_multi)\n",
        "\n",
        "lr_sklearn_multi = LinearRegression()\n",
        "lr_sklearn_multi.fit(X_train_multi, y_train_multi)\n",
        "y_pred_sklearn_multi = lr_sklearn_multi.predict(X_test_multi)\n",
        "\n",
        "print(\"Regresión Lineal Múltiple:\")\n",
        "print(f\"\\nCoeficientes (Ecuación Normal):\")\n",
        "print(f\"  Intercept: {lr_normal_multi.intercept:.4f}\")\n",
        "for i, coef in enumerate(lr_normal_multi.coefficients):\n",
        "    print(f\"  Coeficiente {i+1}: {coef:.4f}\")\n",
        "\n",
        "print(f\"\\nCoeficientes (Scikit-learn):\")\n",
        "print(f\"  Intercept: {lr_sklearn_multi.intercept_:.4f}\")\n",
        "for i, coef in enumerate(lr_sklearn_multi.coef_):\n",
        "    print(f\"  Coeficiente {i+1}: {coef:.4f}\")\n",
        "\n",
        "print(f\"\\nMSE: {mean_squared_error(y_test_multi, y_pred_normal_multi):.4f}\")\n",
        "print(f\"R²: {r2_score(y_test_multi, y_pred_normal_multi):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ventajas y Desventajas\n",
        "\n",
        "**Ventajas:**\n",
        "- Simple de entender e implementar\n",
        "- Muy rápido de entrenar\n",
        "- Interpretable (los coeficientes tienen significado)\n",
        "- No requiere ajuste de hiperparámetros\n",
        "- Funciona bien cuando la relación es lineal\n",
        "\n",
        "**Desventajas:**\n",
        "- Asume relación lineal (puede no funcionar con relaciones no lineales)\n",
        "- Sensible a outliers\n",
        "- Puede tener problemas con multicolinealidad\n",
        "- Requiere normalización para gradiente descendente\n",
        "\n",
        "## Aplicaciones\n",
        "\n",
        "- Predicción de precios\n",
        "- Análisis de tendencias\n",
        "- Modelado de relaciones causa-efecto\n",
        "- Forecasting\n",
        "- Análisis económico\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
