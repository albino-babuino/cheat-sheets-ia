{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# K-Nearest Neighbors (KNN)\n",
        "\n",
        "Este notebook explica el algoritmo K-Nearest Neighbors (KNN), uno de los algoritmos de aprendizaje supervisado más simples e intuitivos.\n",
        "\n",
        "## ¿Qué es KNN?\n",
        "\n",
        "K-Nearest Neighbors es un algoritmo de aprendizaje supervisado que puede usarse tanto para clasificación como para regresión. Es un algoritmo \"lazy\" (perezoso) porque no aprende un modelo explícito, sino que almacena todos los datos de entrenamiento y hace predicciones basándose en la similitud con los ejemplos más cercanos.\n",
        "\n",
        "## Conceptos Fundamentales\n",
        "\n",
        "- **K**: Número de vecinos más cercanos a considerar\n",
        "- **Distancia**: Métrica para medir la similitud entre puntos (Euclidiana, Manhattan, etc.)\n",
        "- **Votación**: En clasificación, la clase más común entre los K vecinos\n",
        "- **Promedio**: En regresión, el promedio de los valores de los K vecinos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementación Básica de KNN\n",
        "\n",
        "El algoritmo KNN funciona de la siguiente manera:\n",
        "\n",
        "1. **Almacenar** todos los datos de entrenamiento\n",
        "2. **Calcular distancias** entre el punto a predecir y todos los puntos de entrenamiento\n",
        "3. **Seleccionar** los K puntos más cercanos\n",
        "4. **Predecir**:\n",
        "   - **Clasificación**: Clase más común entre los K vecinos\n",
        "   - **Regresión**: Promedio de los valores de los K vecinos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KNNClassifier:\n",
        "    \"\"\"Implementación básica de KNN para clasificación\"\"\"\n",
        "    \n",
        "    def __init__(self, k=3):\n",
        "        \"\"\"\n",
        "        Parámetros:\n",
        "        - k: Número de vecinos a considerar\n",
        "        \"\"\"\n",
        "        self.k = k\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Almacena los datos de entrenamiento\"\"\"\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "    \n",
        "    def euclidean_distance(self, x1, x2):\n",
        "        \"\"\"Calcula la distancia euclidiana entre dos puntos\"\"\"\n",
        "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Predice las clases para los puntos en X\"\"\"\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            # Calcular distancias a todos los puntos de entrenamiento\n",
        "            distances = [self.euclidean_distance(x, x_train) \n",
        "                        for x_train in self.X_train]\n",
        "            \n",
        "            # Obtener índices de los k vecinos más cercanos\n",
        "            k_indices = np.argsort(distances)[:self.k]\n",
        "            \n",
        "            # Obtener las clases de los k vecinos más cercanos\n",
        "            k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "            \n",
        "            # Votación: clase más común\n",
        "            most_common = max(set(k_nearest_labels), key=k_nearest_labels.count)\n",
        "            predictions.append(most_common)\n",
        "        \n",
        "        return np.array(predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KNN para Clasificación\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar dataset de iris\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Dividir en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Usar nuestra implementación\n",
        "knn_custom = KNNClassifier(k=3)\n",
        "knn_custom.fit(X_train, y_train)\n",
        "y_pred_custom = knn_custom.predict(X_test)\n",
        "\n",
        "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
        "print(f\"Precisión con implementación propia (k=3): {accuracy_custom:.4f}\")\n",
        "\n",
        "# Usar scikit-learn para comparar\n",
        "knn_sklearn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_sklearn.fit(X_train, y_train)\n",
        "y_pred_sklearn = knn_sklearn.predict(X_test)\n",
        "\n",
        "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
        "print(f\"Precisión con scikit-learn (k=3): {accuracy_sklearn:.4f}\")\n",
        "\n",
        "print(\"\\nReporte de clasificación:\")\n",
        "print(classification_report(y_test, y_pred_sklearn, target_names=iris.target_names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KNN para Regresión\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KNNRegressor:\n",
        "    \"\"\"Implementación básica de KNN para regresión\"\"\"\n",
        "    \n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "    \n",
        "    def euclidean_distance(self, x1, x2):\n",
        "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "    \n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            distances = [self.euclidean_distance(x, x_train) \n",
        "                        for x_train in self.X_train]\n",
        "            k_indices = np.argsort(distances)[:self.k]\n",
        "            k_nearest_values = [self.y_train[i] for i in k_indices]\n",
        "            # Promedio de los k vecinos más cercanos\n",
        "            predictions.append(np.mean(k_nearest_values))\n",
        "        return np.array(predictions)\n",
        "\n",
        "# Ejemplo con datos de regresión\n",
        "X_reg, y_reg = datasets.make_regression(n_samples=100, n_features=1, \n",
        "                                        noise=10, random_state=42)\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Usar nuestra implementación\n",
        "knn_reg_custom = KNNRegressor(k=5)\n",
        "knn_reg_custom.fit(X_train_reg, y_train_reg)\n",
        "y_pred_reg_custom = knn_reg_custom.predict(X_test_reg)\n",
        "\n",
        "# Usar scikit-learn\n",
        "knn_reg_sklearn = KNeighborsRegressor(n_neighbors=5)\n",
        "knn_reg_sklearn.fit(X_train_reg, y_train_reg)\n",
        "y_pred_reg_sklearn = knn_reg_sklearn.predict(X_test_reg)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "mse_custom = mean_squared_error(y_test_reg, y_pred_reg_custom)\n",
        "r2_custom = r2_score(y_test_reg, y_pred_reg_custom)\n",
        "print(f\"Implementación propia - MSE: {mse_custom:.4f}, R²: {r2_custom:.4f}\")\n",
        "\n",
        "mse_sklearn = mean_squared_error(y_test_reg, y_pred_reg_sklearn)\n",
        "r2_sklearn = r2_score(y_test_reg, y_pred_reg_sklearn)\n",
        "print(f\"Scikit-learn - MSE: {mse_sklearn:.4f}, R²: {r2_sklearn:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Efecto del Valor de K\n",
        "\n",
        "El valor de K es crucial para el rendimiento del algoritmo:\n",
        "- **K pequeño**: Más sensible al ruido, puede sobreajustarse\n",
        "- **K grande**: Más suave, pero puede perder detalles importantes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Probar diferentes valores de K\n",
        "k_values = range(1, 21)\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "for k in k_values:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    train_scores.append(knn.score(X_train, y_train))\n",
        "    test_scores.append(knn.score(X_test, y_test))\n",
        "\n",
        "# Visualizar resultados\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(k_values, train_scores, 'o-', label='Entrenamiento', linewidth=2)\n",
        "plt.plot(k_values, test_scores, 's-', label='Prueba', linewidth=2)\n",
        "plt.xlabel('Valor de K')\n",
        "plt.ylabel('Precisión')\n",
        "plt.title('Efecto del Valor de K en el Rendimiento de KNN')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(k_values)\n",
        "plt.show()\n",
        "\n",
        "# Encontrar el mejor K\n",
        "best_k = k_values[np.argmax(test_scores)]\n",
        "print(f\"Mejor valor de K: {best_k} (Precisión: {max(test_scores):.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Métricas de Distancia\n",
        "\n",
        "KNN puede usar diferentes métricas de distancia:\n",
        "\n",
        "- **Euclidiana**: `√(Σ(xi - yi)²)` - Distancia en línea recta\n",
        "- **Manhattan**: `Σ|xi - yi|` - Distancia en cuadrícula\n",
        "- **Minkowski**: Generalización de las anteriores\n",
        "- **Hamming**: Para datos categóricos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparar diferentes métricas de distancia\n",
        "metrics = ['euclidean', 'manhattan', 'minkowski']\n",
        "results = {}\n",
        "\n",
        "for metric in metrics:\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, metric=metric)\n",
        "    knn.fit(X_train, y_train)\n",
        "    score = knn.score(X_test, y_test)\n",
        "    results[metric] = score\n",
        "    print(f\"{metric.capitalize()}: {score:.4f}\")\n",
        "\n",
        "# Visualizar\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(results.keys(), results.values(), color=['skyblue', 'lightgreen', 'salmon'])\n",
        "plt.ylabel('Precisión')\n",
        "plt.title('Comparación de Métricas de Distancia en KNN')\n",
        "plt.ylim([min(results.values()) - 0.01, max(results.values()) + 0.01])\n",
        "for metric, score in results.items():\n",
        "    plt.text(metric, score, f'{score:.4f}', ha='center', va='bottom')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ventajas y Desventajas\n",
        "\n",
        "**Ventajas:**\n",
        "- Simple de entender e implementar\n",
        "- No requiere entrenamiento (lazy learning)\n",
        "- Funciona bien con datos no lineales\n",
        "- Puede usarse para clasificación y regresión\n",
        "\n",
        "**Desventajas:**\n",
        "- Computacionalmente costoso en predicción (calcula todas las distancias)\n",
        "- Sensible a características irrelevantes\n",
        "- Requiere normalización de datos\n",
        "- No funciona bien con datos de alta dimensionalidad (curse of dimensionality)\n",
        "- Sensible a datos desbalanceados\n",
        "\n",
        "## Aplicaciones\n",
        "\n",
        "- Reconocimiento de patrones\n",
        "- Sistemas de recomendación\n",
        "- Clasificación de imágenes\n",
        "- Análisis de datos médicos\n",
        "- Detección de anomalías\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
