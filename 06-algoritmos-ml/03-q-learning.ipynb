{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q-Learning\n",
        "\n",
        "Este notebook explica el algoritmo Q-Learning, uno de los algoritmos más importantes en Reinforcement Learning (Aprendizaje por Refuerzo).\n",
        "\n",
        "## ¿Qué es Q-Learning?\n",
        "\n",
        "Q-Learning es un algoritmo de aprendizaje por refuerzo sin modelo (model-free) que aprende la política óptima de acción mediante la estimación de valores Q (quality). Es un algoritmo de aprendizaje fuera de política (off-policy) que puede aprender la política óptima independientemente de la política que sigue.\n",
        "\n",
        "## Conceptos Fundamentales\n",
        "\n",
        "- **Agente**: Entidad que toma decisiones y realiza acciones\n",
        "- **Ambiente (Environment)**: El mundo en el que el agente actúa\n",
        "- **Estado (State)**: Situación actual del ambiente\n",
        "- **Acción (Action)**: Movimiento que el agente puede realizar\n",
        "- **Recompensa (Reward)**: Feedback numérico que recibe el agente\n",
        "- **Q-Value**: Valor esperado de tomar una acción en un estado dado\n",
        "- **Política (Policy)**: Estrategia que el agente usa para seleccionar acciones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from typing import Tuple, Dict, List\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ecuación de Actualización Q-Learning\n",
        "\n",
        "La ecuación de Bellman para Q-Learning es:\n",
        "\n",
        "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]$$\n",
        "\n",
        "Donde:\n",
        "- $Q(s, a)$: Valor Q del estado $s$ y acción $a$\n",
        "- $\\alpha$ (alpha): Tasa de aprendizaje (learning rate)\n",
        "- $r$: Recompensa inmediata\n",
        "- $\\gamma$ (gamma): Factor de descuento (discount factor)\n",
        "- $s'$: Estado siguiente\n",
        "- $\\max_{a'} Q(s', a')$: Máximo valor Q del estado siguiente\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QLearningAgent:\n",
        "    \"\"\"Agente que aprende usando Q-Learning\"\"\"\n",
        "    \n",
        "    def __init__(self, n_states, n_actions, learning_rate=0.1, \n",
        "                 discount_factor=0.95, epsilon=1.0, epsilon_decay=0.995, \n",
        "                 epsilon_min=0.01):\n",
        "        \"\"\"\n",
        "        Parámetros:\n",
        "        - n_states: Número de estados posibles\n",
        "        - n_actions: Número de acciones posibles\n",
        "        - learning_rate: Tasa de aprendizaje (alpha)\n",
        "        - discount_factor: Factor de descuento (gamma)\n",
        "        - epsilon: Probabilidad inicial de exploración (epsilon-greedy)\n",
        "        - epsilon_decay: Tasa de decaimiento de epsilon\n",
        "        - epsilon_min: Valor mínimo de epsilon\n",
        "        \"\"\"\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        \n",
        "        # Inicializar tabla Q con ceros\n",
        "        self.q_table = np.zeros((n_states, n_actions))\n",
        "    \n",
        "    def choose_action(self, state, training=True):\n",
        "        \"\"\"\n",
        "        Selecciona una acción usando política epsilon-greedy\n",
        "        \n",
        "        - Con probabilidad epsilon: acción aleatoria (exploración)\n",
        "        - Con probabilidad 1-epsilon: mejor acción según Q-table (explotación)\n",
        "        \"\"\"\n",
        "        if training and random.random() < self.epsilon:\n",
        "            # Exploración: acción aleatoria\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        else:\n",
        "            # Explotación: mejor acción\n",
        "            return np.argmax(self.q_table[state])\n",
        "    \n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Actualiza la tabla Q usando la ecuación de Q-Learning\n",
        "        \"\"\"\n",
        "        current_q = self.q_table[state, action]\n",
        "        \n",
        "        if done:\n",
        "            # Si el episodio terminó, no hay estado siguiente\n",
        "            target_q = reward\n",
        "        else:\n",
        "            # Calcular el máximo Q-value del estado siguiente\n",
        "            max_next_q = np.max(self.q_table[next_state])\n",
        "            target_q = reward + self.discount_factor * max_next_q\n",
        "        \n",
        "        # Actualizar Q-value\n",
        "        self.q_table[state, action] = current_q + self.learning_rate * (target_q - current_q)\n",
        "        \n",
        "        # Decaer epsilon\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "    \n",
        "    def get_policy(self):\n",
        "        \"\"\"Retorna la política aprendida (mejor acción para cada estado)\"\"\"\n",
        "        return np.argmax(self.q_table, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejemplo: Laberinto Simple\n",
        "\n",
        "Vamos a implementar Q-Learning para que un agente aprenda a navegar por un laberinto.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MazeEnvironment:\n",
        "    \"\"\"Ambiente de laberinto simple\"\"\"\n",
        "    \n",
        "    def __init__(self, maze_size=5):\n",
        "        \"\"\"\n",
        "        Laberinto representado como una cuadrícula:\n",
        "        - 0: Espacio libre\n",
        "        - 1: Pared\n",
        "        - 2: Meta\n",
        "        \"\"\"\n",
        "        self.maze_size = maze_size\n",
        "        self.maze = np.zeros((maze_size, maze_size))\n",
        "        \n",
        "        # Crear un laberinto simple\n",
        "        # Paredes en algunas posiciones\n",
        "        self.maze[1, 1] = 1\n",
        "        self.maze[1, 2] = 1\n",
        "        self.maze[2, 1] = 1\n",
        "        self.maze[3, 3] = 1\n",
        "        \n",
        "        # Meta en la esquina inferior derecha\n",
        "        self.maze[maze_size-1, maze_size-1] = 2\n",
        "        \n",
        "        # Posición inicial\n",
        "        self.state = 0  # Estado inicial (0, 0)\n",
        "        self.start_state = 0\n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\"Reinicia el ambiente al estado inicial\"\"\"\n",
        "        self.state = self.start_state\n",
        "        return self.state\n",
        "    \n",
        "    def state_to_pos(self, state):\n",
        "        \"\"\"Convierte estado (número) a posición (fila, columna)\"\"\"\n",
        "        return (state // self.maze_size, state % self.maze_size)\n",
        "    \n",
        "    def pos_to_state(self, row, col):\n",
        "        \"\"\"Convierte posición (fila, columna) a estado (número)\"\"\"\n",
        "        return row * self.maze_size + col\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Ejecuta una acción y retorna (next_state, reward, done)\n",
        "        \n",
        "        Acciones:\n",
        "        0: Arriba\n",
        "        1: Derecha\n",
        "        2: Abajo\n",
        "        3: Izquierda\n",
        "        \"\"\"\n",
        "        row, col = self.state_to_pos(self.state)\n",
        "        \n",
        "        # Movimientos: arriba, derecha, abajo, izquierda\n",
        "        moves = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n",
        "        dr, dc = moves[action]\n",
        "        \n",
        "        new_row = row + dr\n",
        "        new_col = col + dc\n",
        "        \n",
        "        # Verificar límites\n",
        "        if new_row < 0 or new_row >= self.maze_size or \\\n",
        "           new_col < 0 or new_col >= self.maze_size:\n",
        "            # Movimiento inválido: quedarse en el mismo lugar\n",
        "            return self.state, -0.1, False\n",
        "        \n",
        "        # Verificar si hay pared\n",
        "        if self.maze[new_row, new_col] == 1:\n",
        "            # Chocar con pared: penalización pequeña\n",
        "            return self.state, -0.5, False\n",
        "        \n",
        "        # Movimiento válido\n",
        "        new_state = self.pos_to_state(new_row, new_col)\n",
        "        self.state = new_state\n",
        "        \n",
        "        # Verificar si llegó a la meta\n",
        "        if self.maze[new_row, new_col] == 2:\n",
        "            return new_state, 10.0, True  # Recompensa grande por llegar a la meta\n",
        "        \n",
        "        # Movimiento normal: pequeña penalización por paso\n",
        "        return new_state, -0.1, False\n",
        "    \n",
        "    def render(self):\n",
        "        \"\"\"Visualiza el laberinto\"\"\"\n",
        "        display = self.maze.copy()\n",
        "        row, col = self.state_to_pos(self.state)\n",
        "        display[row, col] = 3  # Marcar posición del agente\n",
        "        \n",
        "        print(\"Laberinto:\")\n",
        "        for r in range(self.maze_size):\n",
        "            for c in range(self.maze_size):\n",
        "                if display[r, c] == 0:\n",
        "                    print(\".\", end=\" \")\n",
        "                elif display[r, c] == 1:\n",
        "                    print(\"#\", end=\" \")\n",
        "                elif display[r, c] == 2:\n",
        "                    print(\"G\", end=\" \")  # Goal\n",
        "                elif display[r, c] == 3:\n",
        "                    print(\"A\", end=\" \")  # Agent\n",
        "            print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear ambiente y agente\n",
        "env = MazeEnvironment(maze_size=5)\n",
        "n_states = env.maze_size * env.maze_size\n",
        "n_actions = 4  # Arriba, derecha, abajo, izquierda\n",
        "\n",
        "agent = QLearningAgent(\n",
        "    n_states=n_states,\n",
        "    n_actions=n_actions,\n",
        "    learning_rate=0.1,\n",
        "    discount_factor=0.95,\n",
        "    epsilon=1.0,\n",
        "    epsilon_decay=0.995,\n",
        "    epsilon_min=0.01\n",
        ")\n",
        "\n",
        "# Entrenar el agente\n",
        "n_episodes = 500\n",
        "episode_rewards = []\n",
        "episode_lengths = []\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    \n",
        "    while not done and steps < 100:  # Límite de pasos por episodio\n",
        "        action = agent.choose_action(state, training=True)\n",
        "        next_state, reward, done = env.step(action)\n",
        "        agent.update(state, action, reward, next_state, done)\n",
        "        \n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "    \n",
        "    episode_rewards.append(total_reward)\n",
        "    episode_lengths.append(steps)\n",
        "    \n",
        "    if (episode + 1) % 50 == 0:\n",
        "        avg_reward = np.mean(episode_rewards[-50:])\n",
        "        avg_length = np.mean(episode_lengths[-50:])\n",
        "        print(f\"Episodio {episode + 1}: Recompensa promedio = {avg_reward:.2f}, \"\n",
        "              f\"Pasos promedio = {avg_length:.1f}, Epsilon = {agent.epsilon:.3f}\")\n",
        "\n",
        "print(\"\\nEntrenamiento completado!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizar el aprendizaje\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Recompensas por episodio\n",
        "ax1.plot(episode_rewards, alpha=0.3, color='blue', label='Recompensa por episodio')\n",
        "# Media móvil\n",
        "window = 50\n",
        "if len(episode_rewards) >= window:\n",
        "    moving_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
        "    ax1.plot(range(window-1, len(episode_rewards)), moving_avg, \n",
        "             color='red', linewidth=2, label=f'Media móvil ({window} episodios)')\n",
        "ax1.set_xlabel('Episodio')\n",
        "ax1.set_ylabel('Recompensa')\n",
        "ax1.set_title('Recompensas durante el Entrenamiento')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Longitud de episodios\n",
        "ax2.plot(episode_lengths, alpha=0.3, color='green', label='Pasos por episodio')\n",
        "if len(episode_lengths) >= window:\n",
        "    moving_avg = np.convolve(episode_lengths, np.ones(window)/window, mode='valid')\n",
        "    ax2.plot(range(window-1, len(episode_lengths)), moving_avg, \n",
        "             color='orange', linewidth=2, label=f'Media móvil ({window} episodios)')\n",
        "ax2.set_xlabel('Episodio')\n",
        "ax2.set_ylabel('Pasos')\n",
        "ax2.set_title('Longitud de Episodios durante el Entrenamiento')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Probar el agente entrenado\n",
        "print(\"Probando el agente entrenado:\")\n",
        "state = env.reset()\n",
        "env.render()\n",
        "print()\n",
        "\n",
        "steps = 0\n",
        "done = False\n",
        "path = [env.state_to_pos(state)]\n",
        "\n",
        "while not done and steps < 20:\n",
        "    action = agent.choose_action(state, training=False)  # Sin exploración\n",
        "    next_state, reward, done = env.step(action)\n",
        "    state = next_state\n",
        "    path.append(env.state_to_pos(state))\n",
        "    steps += 1\n",
        "    \n",
        "    print(f\"Paso {steps}: Acción = {['Arriba', 'Derecha', 'Abajo', 'Izquierda'][action]}\")\n",
        "    env.render()\n",
        "    print()\n",
        "    \n",
        "    if done:\n",
        "        print(f\"¡Meta alcanzada en {steps} pasos!\")\n",
        "        break\n",
        "\n",
        "print(f\"Camino seguido: {path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualización de la Tabla Q\n",
        "\n",
        "Podemos visualizar los valores Q aprendidos para entender qué acciones prefiere el agente en cada estado.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizar la tabla Q\n",
        "print(\"Tabla Q (valores para cada estado-acción):\")\n",
        "print(\"Estados son filas, acciones son columnas [Arriba, Derecha, Abajo, Izquierda]\")\n",
        "print(\"\\n\", agent.q_table)\n",
        "\n",
        "# Visualizar la política aprendida\n",
        "policy = agent.get_policy()\n",
        "print(\"\\nPolítica aprendida (mejor acción para cada estado):\")\n",
        "action_names = ['↑', '→', '↓', '←']\n",
        "for state in range(n_states):\n",
        "    row, col = env.state_to_pos(state)\n",
        "    if env.maze[row, col] == 1:  # Pared\n",
        "        print(\"#\", end=\" \")\n",
        "    elif env.maze[row, col] == 2:  # Meta\n",
        "        print(\"G\", end=\" \")\n",
        "    else:\n",
        "        print(action_names[policy[state]], end=\" \")\n",
        "    \n",
        "    if (state + 1) % env.maze_size == 0:\n",
        "        print()  # Nueva línea al final de cada fila\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parámetros Importantes\n",
        "\n",
        "### Learning Rate (α)\n",
        "- Controla qué tan rápido aprende el agente\n",
        "- Valores altos: aprendizaje rápido pero inestable\n",
        "- Valores bajos: aprendizaje lento pero estable\n",
        "- Típico: 0.1 - 0.3\n",
        "\n",
        "### Discount Factor (γ)\n",
        "- Determina la importancia de recompensas futuras\n",
        "- Valores cercanos a 1: el agente valora recompensas futuras\n",
        "- Valores cercanos a 0: el agente solo valora recompensas inmediatas\n",
        "- Típico: 0.9 - 0.99\n",
        "\n",
        "### Epsilon (ε) - Exploración vs Explotación\n",
        "- Epsilon-greedy: balance entre exploración y explotación\n",
        "- Alto epsilon: más exploración (aprende más)\n",
        "- Bajo epsilon: más explotación (usa lo aprendido)\n",
        "- Epsilon decay: reduce exploración con el tiempo\n",
        "\n",
        "## Ventajas y Desventajas\n",
        "\n",
        "**Ventajas:**\n",
        "- No requiere modelo del ambiente (model-free)\n",
        "- Puede aprender la política óptima sin seguirla (off-policy)\n",
        "- Relativamente simple de implementar\n",
        "- Funciona bien en espacios de estados discretos pequeños\n",
        "\n",
        "**Desventajas:**\n",
        "- No escala bien a espacios de estados grandes (curse of dimensionality)\n",
        "- Requiere discretización para estados continuos\n",
        "- Puede ser lento en convergencia\n",
        "- No maneja bien la aleatoriedad en transiciones\n",
        "\n",
        "## Aplicaciones\n",
        "\n",
        "- Juegos (ajedrez, damas, videojuegos)\n",
        "- Robótica (navegación, control)\n",
        "- Sistemas de recomendación\n",
        "- Trading algorítmico\n",
        "- Optimización de recursos\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
