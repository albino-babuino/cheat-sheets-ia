{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Árboles de Decisión\n",
        "\n",
        "Este notebook explica el algoritmo de Árboles de Decisión, uno de los algoritmos más intuitivos y versátiles en Machine Learning.\n",
        "\n",
        "## ¿Qué es un Árbol de Decisión?\n",
        "\n",
        "Un árbol de decisión es un modelo de aprendizaje supervisado que utiliza una estructura de árbol para tomar decisiones. Cada nodo interno representa una prueba sobre un atributo, cada rama representa el resultado de la prueba, y cada hoja (nodo terminal) representa una clase o valor.\n",
        "\n",
        "## Ventajas y Desventajas\n",
        "\n",
        "**Ventajas:**\n",
        "- Fácil de entender e interpretar\n",
        "- Requiere poca preparación de datos\n",
        "- Puede manejar datos numéricos y categóricos\n",
        "- No requiere normalización de datos\n",
        "\n",
        "**Desventajas:**\n",
        "- Puede sobreajustarse fácilmente (overfitting)\n",
        "- Puede ser inestable (pequeños cambios en datos pueden generar árboles muy diferentes)\n",
        "- Sesgo hacia características con más niveles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conceptos Fundamentales\n",
        "\n",
        "### 1. Entropía\n",
        "La entropía mide la impureza o desorden en un conjunto de datos. Se calcula como:\n",
        "\n",
        "$$H(S) = -\\sum_{i=1}^{c} p_i \\log_2(p_i)$$\n",
        "\n",
        "donde $p_i$ es la proporción de ejemplos de la clase $i$ en el conjunto $S$.\n",
        "\n",
        "### 2. Ganancia de Información\n",
        "La ganancia de información mide la reducción en entropía después de dividir un conjunto según un atributo:\n",
        "\n",
        "$$IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)$$\n",
        "\n",
        "### 3. Índice Gini\n",
        "El índice Gini es otra medida de impureza:\n",
        "\n",
        "$$Gini(S) = 1 - \\sum_{i=1}^{c} p_i^2$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejemplo de cálculo de entropía\n",
        "def entropia(y):\n",
        "    \"\"\"Calcula la entropía de un conjunto de etiquetas\"\"\"\n",
        "    if len(y) == 0:\n",
        "        return 0\n",
        "    # Contar frecuencias de cada clase\n",
        "    counts = np.bincount(y)\n",
        "    # Calcular proporciones\n",
        "    proportions = counts / len(y)\n",
        "    # Eliminar ceros para evitar log(0)\n",
        "    proportions = proportions[proportions > 0]\n",
        "    # Calcular entropía\n",
        "    return -np.sum(proportions * np.log2(proportions))\n",
        "\n",
        "# Ejemplo de cálculo de índice Gini\n",
        "def gini(y):\n",
        "    \"\"\"Calcula el índice Gini de un conjunto de etiquetas\"\"\"\n",
        "    if len(y) == 0:\n",
        "        return 0\n",
        "    counts = np.bincount(y)\n",
        "    proportions = counts / len(y)\n",
        "    return 1 - np.sum(proportions ** 2)\n",
        "\n",
        "# Ejemplo\n",
        "y_ejemplo = np.array([0, 0, 0, 1, 1, 1, 1, 1])\n",
        "print(f\"Entropía: {entropia(y_ejemplo):.4f}\")\n",
        "print(f\"Gini: {gini(y_ejemplo):.4f}\")\n",
        "\n",
        "# Conjunto más puro\n",
        "y_puro = np.array([0, 0, 0, 0, 0, 0])\n",
        "print(f\"\\nConjunto puro - Entropía: {entropia(y_puro):.4f}, Gini: {gini(y_puro):.4f}\")\n",
        "\n",
        "# Conjunto más impuro (50-50)\n",
        "y_impuro = np.array([0, 0, 0, 1, 1, 1])\n",
        "print(f\"Conjunto impuro - Entropía: {entropia(y_impuro):.4f}, Gini: {gini(y_impuro):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Árbol de Decisión para Clasificación\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar dataset de iris\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Dividir en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Crear y entrenar el árbol de decisión\n",
        "dt_clf = DecisionTreeClassifier(\n",
        "    criterion='gini',        # Criterio de división: 'gini' o 'entropy'\n",
        "    max_depth=3,             # Profundidad máxima del árbol\n",
        "    min_samples_split=2,     # Mínimo de muestras para dividir un nodo\n",
        "    min_samples_leaf=1,      # Mínimo de muestras en una hoja\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "dt_clf.fit(X_train, y_train)\n",
        "\n",
        "# Hacer predicciones\n",
        "y_pred = dt_clf.predict(X_test)\n",
        "\n",
        "# Evaluar el modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Precisión: {accuracy:.4f}\")\n",
        "print(\"\\nReporte de clasificación:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizar el árbol de decisión\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(dt_clf, \n",
        "          feature_names=iris.feature_names,\n",
        "          class_names=iris.target_names,\n",
        "          filled=True,\n",
        "          rounded=True,\n",
        "          fontsize=10)\n",
        "plt.title(\"Árbol de Decisión - Dataset Iris\", fontsize=16)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
